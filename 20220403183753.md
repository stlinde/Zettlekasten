# A Crash Course in Compilers
**Author**: Ramsey Nasser
**Link**: [A crash course in compilers (https://increment.com/programming-languages/crash-course-in-compilers/)

Programming languages asks us to reshape our minds, and that is what makes them deeply personal and subjective.
Few software engineers develop a deep relationship with PLT (Programming Language Theory).
Diving deeper into PLT is a great way to grow as a developer.

An interpreter's job is to take source code and immediately implement its effects.
The advantages of interpreters include their simplicity, the fact that they can often start executing faster than compilers, and their ability to run in environments where compiling new code is prohibited.

The job of a compiler is to take source code and translate it into a target code with the same meaning.
The generated target code can then be evaluated in order to carry out the computation of the original source code.
C is compiled into machine code that a computer's hardware can execute directly.
C# and Java are similar, but they compile into bytecodes that are executed by the Common Language Runtime (CLR) or the Java virtual machine (JVM), respectively, as opposed to physical hardware.

## How does a compiler work?
Compilers proceed in a linear sequence of phases, a pipeline.

### Parsing
Parsing takes the user prepared source code and turn it into a data structure that is useful at the later stages of compilation.
This is the stage where syntax errors are reported.

Lisp reading (parsing) is the conversion that happens from Lisp source code to data structures that the language understands.

The majority of mainstream languages require a two-step process: scanning followed by parsing.
A scanner (also known as a lexical analyzer) reads source text and produces a linear stream of tokens.
The parser reads the stream of tokens and recognizes patterns to transform into nodes in an abstract syntax tree that the next step of the pipeline will deal with.

### Analysis
Once parsing is complete, the compiler must analyze the parsed code into an abstract syntax tree, or AST.
Analysis answers the question, "What did the user mean?".
This is where the semantic features of the language are implemented, like name resolution, control flow, and function invocation.
Additionally, analysis is a phase where optimizations can begin to happen, by transforming the AST into semantically equivalent ASTs that perform better.

In languages with types, this is where type information is inferred, flowed, and validated. 

### Emission
Once an AST is produced and settled on, the final step is to emit the target code.
When targeting machine code, modern languages will most often use the LLVM toolchain. 
Virtual machine targets like the CLR and the JVM are similar, but each exposes a bytecode language that is at an even higher level than LLVM I .


### Tooling and Ecosystems
Once a language's compiler is working, the question then becomes one of editor integration, debugger support, documentation, a community, and a library ecosystem.,
Most of this takes considerable time to develop, and this is what gives existing languages inertia over new ones.

## Why anyone would do this
It is a wonderful puzzle to solve, and more approachable than it might seem at first.
Languages represent different ideas of how to capture human creativity on a machine.
Seeing common patterns across different languages and getting a sense fo their trade-offs also gives you a new perspective when picking up new languages, something every working programmer will have to do at some point in their career.
The world of programming languages will expand your mind and make you a better programmer.

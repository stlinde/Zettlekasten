# Neural Networks, Types, and Functional Programming
**Author**: Christoph Olah

**Link**: [Neural Networks, Types, and Functional Programming](https://colah.github.io/posts/2015-09-NN-Types-FP/)

## An Ad-Hoc Field
Deep learning is a young field.
Young fields often start in an ad-hoc manner.
Later, the mature field is understood very differently than it was early on.
Deep learning is currently held together by a very successful tool we stumbled on rather randomly. 

## Deep Learning 30 Years in the Future
Three narratives competing to be the way we understand deep learning:
* Neuroscience - analogies to biology.
* Representations - transformations of data and the manifold hypothesis.
* Probabilistic - neural networks as finding latent variables.

This essay extends the representations narrative to a new answer: deep learning studies a connection between optimization and functional programming.

In this view, the representations narrative in deep learning corresponds to type theory in functional programming.

## Optimization and Function Composition
The distinctive property of deep learning is that it studies deep neural networks.
Over the course of multiple layers, these models progressively bend data, warping it into a form where it is easy to solve the given task. 

Each layer is a function, acting on the output of a previous layer.
As a whole, the network is a chain of composed functions.
This chain of composed functions is optimized to perform a task.

## Representations are Types
With every layer, neural networks transform data, modeling it into a form that makes their task easier to do.
We call these transformed versions of data "representations".

Representations correspond to types.

At their crudest, types in computer science are a way of embedding some kind of data in *n* bits.
Similarly, representations in deep learning are a way to embed a data manifold in *n* dimensions.

Over the course of training, adjacent layers negotiate representations they will communicate in; the performance of the network depends on data being in the represnetation the network expects.

Representations and types can be seen as the basic building blocks for deep learning and functional programming respectively.
One of the major narratives of deep learning, the manifolds and representations narrative, is entirely centered on neural networks bending data into new representations.
The known connection between geometry, logic, topology and functional programming suggests that the connection between representations and types may be of fundamental significane.

## Deep Learning & Functional Programming
One of the key insights behind modern neural networks is the idea that many copies of one neuran can be used in a neural network.
This maps to functions being used over and over again in programming.
Using multiple copies of a neuron in different places is the neural network equivalent of using functions.
Because there is less to learn, the model learns more quickly and learns a better model.
This technique - 'weight tying' - is essential for the phenomenal results we've recently seen from deep learning.

For the model to work we have to do it in a principled way, exploiting some structure in the data - recurrent layers or convolutional layers.

These neural network patterns are just higher order functions - that is, functions which take functions as arguments.
Many of these network patterns correspond to extremely common functions, like `fold`. 
The only unusual thing is that, instead of receiving normal functions as arguments, they receive chunks of neural networks.

**Encoding Recurrent Neural Networks** are just folds. They are often used to allow a neural network to take a variable length list as input. The Haskell version is: `foldl a s`.


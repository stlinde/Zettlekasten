# A Gentle Introduction to Maximum Likelihood Estimation
**Author**: Jonathan Balaban

**Link**: [A Gentle Introduction to Maximum Likelihood](https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-9fbff27ea12f)

The distinction between probability and likelihood is fundamentally important: Probability attaches to possible results; likelihood attaches to hypotheses.

The probability of a set of outcomes will always sum to one.
They are mutually exclusive and exhaustive.

This is not so with likelihoods, as we may have infinitely many hypotheses about the data and the data generating process.
Thus, the likelihood is not mutually exclusive or exhaustive.

We approach the MLE from the Bayesian angle.
Bayes Theorem states that `P(ß|y) = P(y|ß) * P(ß) / P(y)` -> `posterior = likelihood x prior / evidence`. 

We can effectively ignore the `prior` and the `evidence` as all coefficients are equally likely and the probability of all data values (assume continuous) are equally likely and basically zero.
Thus, we end with `posterior = likelihood`, or `P(ß|y) = P(y|ß)`.

Multiple independent can be multiplied so:
`L(ß|y1, y2, y3, ..., yn) = p(y1|ß)*p(y2|ß)*p(y3|ß)* ... * p(yn|ß) = product(yi|ß)`

This is a function that can be maximized.
But we can take the log of the above product and make it easier:
`max(log(product(p(yi|ß)))) = max(sum(log(p(yi|ß))))`
This is out final cost function of the maximum likelihood estimation.

Usually we are minimizing the negative log likelihood, that is:
`min(-(sum(log(p(yi|ß))))`.

We use maximum likelihood estimation as it is generalizable for regression and classification.
Further, it is efficient; no consistent estimator has lower asymptotic error than MLE if you are using the right distribution.

**Four major steps in applying MLE**:
* Define the likelihood, ensuring you're using the correct distribution for you regression or classification problem.
* Take the natural log and reduce the product function to a sum function.
* Maximize - or minimize the negative of - the objective function.
* Verify that uniform priors are a safe assumption. Otherwise, you could attribute the data to a generating function or model for the world that fails the Law of Parsimony.

